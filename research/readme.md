# research-awesome-list [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

List of machine learning repositories and publication algorithms implementation. Inspired by
[`awesome-machine-learning`](https://github.com/josephmisiti/awesome-machine-learning)

If you want to contribute to this list (please do), just open a pull request.
Also, a listed repository should be deprecated if:

- Repository's owner explicitly say that "this library is not maintained".
- Not committed for long time (2~3 years).

# Table of Contents

- [Repositories](#repositories)

    - [Neural style transfer](#neural-style-transfer)
    - [Model compression / knowledge distillation](#model-compression)
    - [Attention](#attention)


# Repositories

## Neural style transfer

- [Real Time Style Transfer](https://github.com/jsigee87/real-time-style-transfer) - A real time neural style transfer project for EE 461P Data Science Principles, at the University of Texas at Austin. This code is associated with the blog post [Real Time Video Neural Style Transfer](https://towardsdatascience.com/real-time-video-neural-style-transfer-9f6f84590832).

## Model compression
Here is a larger list of relevant publications and repositories: [Awesome list knowledge distillation](https://github.com/dkozlov/awesome-knowledge-distillation).

- [Rocket Launching](https://github.com/zhougr1993/Rocket-Launching) - PyTorch - A universal and efficient framework for training well-performing light net [[paper]](https://arxiv.org/abs/1708.04106) 

- [Knowledge Distillation PyTorch](https://github.com/peterliht/knowledge-distillation-pytorch) - PyTorch - An implementation for exploring deep and shallow knowledge distillation (KD) experiments with flexibility

- [Data-free Knowledge Distillation for Deep Neural Networks](https://github.com/iRapha/replayed_distillation) - Tensorflow - Implementation of Data-free Knowledge Distillation for Deep Neural Networks.

- [Knowledge distillation with Keras](https://github.com/TropComplique/knowledge-distillation-keras) - Keras - Keras implementation of Hinton's knowledge distillation (KD), a way of transferring knowledge from a large model into a smaller model.

- [Quantized distillation](https://github.com/antspy/quantized_distillation) - PyTorch - An implementation of model compression and quantized distillation for the paper [[paper]](https://arxiv.org/abs/1802.05668)
## Attention

- [Attention Transfer](https://github.com/szagoruyko/attention-transfer) - PyTorch - code for "Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer" [[paper]](https://arxiv.org/abs/1612.03928)
